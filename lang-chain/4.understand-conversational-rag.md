### reference 
- https://python.langchain.com/v0.2/docs/tutorials/qa_chat_history/

##### PREREQUISITES 
- Chat History : https://python.langchain.com/v0.2/docs/concepts/#chat-history
- Chat models : https://python.langchain.com/v0.2/docs/concepts/#chat-models
  - Language models로써, 메세지들을 인풋으로 받고, 챗 메세지들을 아웃풋으로 내보내는 형태
  - LLM에 비해 비교적 새로운 모델로써, System 메세지, 사용자 메세지, AI 메세지를 구분할수 있게 도와줌
- LLMs (Large Language Models) : https://python.langchain.com/v0.2/docs/concepts/#llms
  - Chat models에 비해 오래된 모델, 문자열을 입력으로 받고 출력으로 보내는 모델
  - 인간의 언어를 이해하고 생성하기 위해서 거대한 양의 데이터가 학습된 AI 모델, (질문과 답, 요약, 번역, 완성 등을 수행한다)
- Embeddings : https://python.langchain.com/v0.2/docs/concepts/#embedding-models
  - Embeddings 모델은 text의 vector 표현을 생성한다. 벡터는 글자의 의미론적 의미를 담고 있는 숫자의 배열이다. 이 방법으로, 가장 비슷한 의미를 가지는 문자를 찾는 수학적 검색에 이용될 수 있다.
- Vector stores : https://python.langchain.com/v0.2/docs/concepts/#vector-stores
  - 비정형화된 데이터를 저장하고 검색하는 일반적인 방법은 이를 임베딩하고 임베딩한 결과를 저장하는 것. 구조화 되지 않은 query를 임베딩 하고, 그 값에 가장 가까운 임베딩된 벡터를 되돌려 받는다.
  - vector stores는 임베디드된 백터를 저장하고, 백터 검색을 수행한다.
  - vector stores은 metadata도 갖고 있어서, 검색전에 filtering 수행이 가능함
- Retrieval-augmented generation : https://python.langchain.com/v0.2/docs/tutorials/rag/
  - LLMs 으로 만들 수 있는 정교한 어플리케이션 중 하나가 Q&A chatbot 인데, 이런 어플리케이션은 RAG(Retrieval Augmented Generation) 라고 알려진 테크니크를 사용한다. 
- Tools : https://python.langchain.com/v0.2/docs/concepts/#tools
  - 툴은 모델에 의해서 호출 될 수 있게 만들어진 utilities 이다.
  - 입력은 모델에 의해 생성되고, 결과 또한 모델로 전달되도록 디자인 되어있다.
  - 모델이 코드 일부를 컨트롤 할 때나, 외부 API 호출을 원할 때 툴이 사용 되어 진다.
  - 툴은 이름, 어떤 일을 하는지, input 포맷(json), 함수 로 구성된 값을 모델에게 전달 해야 하며, 툴이 모델에 바인딩 될 때, 컨텍스트 로써 해당 정보를 전달 한다. 
  - 1) API 호출에 대해 튜닝 받은 모델은 툴을 더 잘 사용한다. 2) 툴의 이름, 어떤일을 하는지, input 스키마가 잘 정의되어 모델에게 넘겨질 수록 모델이 툴을 선택하기 좋다 3) 복작한 스콥의 툴 보다 단순한 스콥의 툴이 모델이 사용하기 더 쉽다. 
- Agents : https://python.langchain.com/v0.2/docs/concepts/#agents
  - LLM을 추론(reasoning) 엔진으로 삼아서 특정 인풋에 무엇을 수행해야 할지 고려하는 시스템이다. 결과가 다시 agent 안으로 들어가서 액션을 더 수행해야 할지 그만해야 할지 결정을 한다.
  - Lang Graph 는 매우 컨트롤이 잘 되고, 커스터마이즈된 agent를 생성하는것에 목표를 둔 확장 시스템이다. 
  - AgentExecutor 는 레거시 시스템이다.
  - ReAct agents : agent를 설계 하는 인기 있는 구조이다. 반복적인 프로세스에서 reasoning 과 acting 을 결합했다.
    -  model인 인풋과 이전에 일어난 일을 기반으로 어떤 응답을 취할지 생각한다.
    -  model은 가능한 툴에서 하나의 액션을 취한다.
    -  model은 tool을 아규먼트를 생성한다.
    -  agent runtime (executor) 은 선택한 tool을 구문 분석하고, 생성한 arguments를 이용해서 호출을 한다.
    -  executor은 model 에게 tool의 결과를 반환한다.
    -  agent가 결과를 선택하기 전까지 이 과정은 반복 된다. 

### 작업 내용
1. chat history 기반의 context를 고려한 User query가 될 수 있도록 history_aware_retriever 를 정의하고 체인을 구성한다.
2. chat history 기반의 context를 고려한 question_and_aunswer chain 을 구성하여, llm이 retriever가 반환한 문서와 context를 고려한 정답을 만들 수 있게 한다. 
3. RunnableWithMessageHistory 를 이용하면 session id를 두고 챗 히스토리를 구분할 수 있고, history를 관리할 수 있다. 
4. RunnableWithMessageHistory 사용 대신 agent를 사용하여 agent가 적당한 tool을 골라서 사용할 수 있게 할 수 있다. 많은 agent는 자체적으로 채팅 기록을 하고 있어 간단한 경우 RunnableWithMessageHistory 를 이용하지 않아도 된다.


#### RunnableWithMessageHistory 를 이용하는 방법
1. Chains, in which we always execute a retrieval step;

```python
import bs4
import os
from dotenv import load_dotenv, dotenv_values
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_chroma import Chroma
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter

env_path = '.env'
env_vars = dotenv_values(env_path)
for key in env_vars:
    if key in os.environ:
        del os.environ[key]

# Explicitly reload the .env file
load_dotenv(env_path)

os.environ["LANGCHAIN_PROJECT"] = "conversational-test"
print(os.environ["LANGCHAIN_PROJECT"])


llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)


### Construct retriever ###
loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("post-content", "post-title", "post-header")
        )
    ),
)
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)
vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())
retriever = vectorstore.as_retriever()


### Contextualize question ###
contextualize_q_system_prompt = (
    "Given a chat history and the latest user question "
    "which might reference context in the chat history, "
    "formulate a standalone question which can be understood "
    "without the chat history. Do NOT answer the question, "
    "just reformulate it if needed and otherwise return it as is."
)
contextualize_q_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", contextualize_q_system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)
history_aware_retriever = create_history_aware_retriever(
    llm, retriever, contextualize_q_prompt
)


### Answer question ###
system_prompt = (
    "You are an assistant for question-answering tasks. "
    "Use the following pieces of retrieved context to answer "
    "the question. If you don't know the answer, say that you "
    "don't know. Use three sentences maximum and keep the "
    "answer concise."
    "\n\n"
    "{context}"
)
qa_prompt = ChatPromptTemplate.from_messages(
    [
        ("system", system_prompt),
        MessagesPlaceholder("chat_history"),
        ("human", "{input}"),
    ]
)
question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)

rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)


### Statefully manage chat history ###
store = {}


def get_session_history(session_id: str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]


conversational_rag_chain = RunnableWithMessageHistory(
    rag_chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
    output_messages_key="answer",
)

conversational_rag_chain.invoke(
    {"input": "What is Task Decomposition?"},
    config={
        "configurable": {"session_id": "abc123"}
    },  # constructs a key "abc123" in `store`.
)["answer"]


conversational_rag_chain.invoke(
    {"input": "What are common ways of doing it?"},
    config={"configurable": {"session_id": "abc123"}},
)["answer"]


```

#### agent 이용하는 방법
2. Agents, in which we give an LLM discretion over whether and how to execute a retrieval step (or multiple steps).

```python
import bs4
import os
from dotenv import load_dotenv, dotenv_values
from langchain.chains import create_history_aware_retriever, create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_chroma import Chroma
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_community.document_loaders import WebBaseLoader
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.messages import AIMessage, HumanMessage
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain.tools.retriever import create_retriever_tool
from langgraph.checkpoint.sqlite import SqliteSaver
from langgraph.graph import END, StateGraph

env_path = '.env'
env_vars = dotenv_values(env_path)
for key in env_vars:
    if key in os.environ:
        del os.environ[key]

# Explicitly reload the .env file
load_dotenv(env_path)

os.environ["LANGCHAIN_PROJECT"] = "conversational-test"
print(os.environ["LANGCHAIN_PROJECT"])


llm = ChatOpenAI(model="gpt-3.5-turbo", temperature=0)

### Construct retriever ###
loader = WebBaseLoader(
    web_paths=("https://lilianweng.github.io/posts/2023-06-23-agent/",),
    bs_kwargs=dict(
        parse_only=bs4.SoupStrainer(
            class_=("post-content", "post-title", "post-header")
        )
    ),
)
docs = loader.load()

text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
splits = text_splitter.split_documents(docs)
vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())
retriever = vectorstore.as_retriever()


with SqliteSaver.from_conn_string(":memory:") as memory:
   ### Build retriever tool ###
    tool = create_retriever_tool(
        retriever,
        "blog_post_retriever",
        "Searches and returns excerpts from the Autonomous Agents blog post.",
    )
    tools = [tool]

    agent_executor = create_react_agent(llm, tools, checkpointer=memory)
    
    query = "What is Task Decomposition?"

    config = {"configurable": {"thread_id": "abc123"}}
    
    for s in agent_executor.stream(
        {"messages": [HumanMessage(content="Hi! I'm bob")]}, config=config
    ):
        print(s)
        print("----")

    query = "What is Task Decomposition?"

    for s in agent_executor.stream(
        {"messages": [HumanMessage(content=query)]}, config=config
    ):
        print(s)
        print("----")
        
```
 - agent 를 이용하면, 자체 memory 기능이 있어서 history 저장이 된다. 