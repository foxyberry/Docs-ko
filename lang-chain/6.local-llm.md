참고 
 - https://python.langchain.com/v0.2/api_reference/ollama/chat_models/langchain_ollama.chat_models.ChatOllama.html#langchain_ollama.chat_models.ChatOllama
 - https://python.langchain.com/v0.2/docs/integrations/llms/ollama/
 - https://python.langchain.com/v0.2/docs/how_to/local_llms/


## llama3.1 서버 띄우기 
```sh
#ollama pull llama3.1:8b
#ollama list
```


```python
from langchain_ollama import ChatOllama
from langchain_core.output_parsers import StrOutputParser

### prompt 
prompt = ChatPromptTemplate.from_template(
    """ 
    input 
    {input}
    """
)


# LLM
llama = ChatOllama(model="llama3.1:8b", base_url="http://127.0.0.1:11434")
llama_chain = prompt | llama

input=""" test """


response = llama_chain.invoke({"input" : input})
print(response.content)

```

- 서버가 띄어져 있어야 함
- llama는 한국어가 지원이 안됨